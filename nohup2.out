INFO:root:called-params configs/in1k_vith14_ep300_fextrc.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 1,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'image_folder': 'LifeCLEFPlant2022/',
                'num_workers': 32,
                'pin_mem': True,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': True},
    'optimization': {   'ema': [0.996, 1.0],
                        'epochs': 350,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 40,
                        'weight_decay': 0.04}}
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
  )
  (blocks): ModuleList(
    (0-31): 32 x Block(
      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1280, out_features=5120, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
)
Target Encoder: 
VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
  )
  (blocks): ModuleList(
    (0-31): 32 x Block(
      (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1280, out_features=3840, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1280, out_features=1280, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=1280, out_features=5120, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
)
Predictor model:
VisionTransformerPredictor(
  (predictor_embed): Linear(in_features=1280, out_features=384, bias=True)
  (predictor_blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (predictor_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (predictor_proj): Linear(in_features=384, out_features=1280, bias=True)
)
INFO:root:making imagenet data transforms
INFO:root:data-path /home/rtcalumby/adam/luciano/LifeCLEFPlant2022/train/
INFO:root:Initialized PC2022
INFO:root:PlantCLEF dataset created
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:Encountered exception when loading checkpoint 'NoneType' object has no attribute 'load_state_dict'
/home/rtcalumby/miniconda3/envs/py382/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Image(s) Shape: torch.Size([1, 3, 224, 224])
Mask Encoder Shape: [tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
          14,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,
          29,  30,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,
          44,  45,  46,  54,  55,  56,  57,  58,  59,  60,  61,  62,  74,  75,
          76,  77,  78,  91,  92,  93,  94, 107, 108, 109, 110, 123, 124, 125,
         126, 139, 140, 141, 142, 155, 156, 157, 158, 171, 172, 173, 174, 176,
         177, 187, 188, 189, 190, 192, 193, 203, 204, 205, 206, 208, 209, 216,
         217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231,
         232, 233, 234, 235, 236, 237, 238]], device='cuda:0')] Length: 1
Mask Predictor Shape: [tensor([[ 85,  86,  87,  88,  89,  90, 101, 102, 103, 104, 105, 106, 117, 118,
         119, 120, 121, 122, 133, 134, 135, 136, 137, 138, 149, 150, 151, 152,
         153, 154, 165, 166, 167, 168, 169, 170, 181, 182, 183, 184, 185, 186,
         197, 198, 199, 200, 201, 202]], device='cuda:0'), tensor([[ 48,  49,  50,  51,  52,  53,  64,  65,  66,  67,  68,  69,  80,  81,
          82,  83,  84,  85,  96,  97,  98,  99, 100, 101, 112, 113, 114, 115,
         116, 117, 128, 129, 130, 131, 132, 133, 144, 145, 146, 147, 148, 149,
         160, 161, 162, 163, 164, 165]], device='cuda:0'), tensor([[ 98,  99, 100, 101, 102, 103, 114, 115, 116, 117, 118, 119, 130, 131,
         132, 133, 134, 135, 146, 147, 148, 149, 150, 151, 162, 163, 164, 165,
         166, 167, 178, 179, 180, 181, 182, 183, 194, 195, 196, 197, 198, 199,
         210, 211, 212, 213, 214, 215]], device='cuda:0'), tensor([[ 68,  69,  70,  71,  72,  73,  84,  85,  86,  87,  88,  89, 100, 101,
         102, 103, 104, 105, 116, 117, 118, 119, 120, 121, 132, 133, 134, 135,
         136, 137, 148, 149, 150, 151, 152, 153, 164, 165, 166, 167, 168, 169,
         180, 181, 182, 183, 184, 185]], device='cuda:0')] Length: 4
Z: tensor([[[-1.8921e-02,  2.0630e-02, -9.4531e-01,  ...,  1.9629e-01,
           1.6846e-02, -1.2500e-01],
         [ 1.5747e-02,  2.6367e-02, -9.6484e-01,  ...,  1.3477e-01,
           2.5879e-02, -1.7285e-01],
         [ 2.2461e-02,  2.4414e-02, -6.7578e-01,  ...,  6.4941e-02,
           2.8442e-02, -2.7344e-01],
         ...,
         [-4.9072e-02,  1.0864e-02,  1.3867e-01,  ...,  3.0469e-01,
           4.8828e-02, -5.2795e-03],
         [-6.2500e-02, -6.2866e-03,  3.9844e-01,  ...,  1.9409e-02,
           3.1738e-02, -7.8613e-02],
         [-8.5449e-02, -1.5381e-02,  8.2031e-01,  ..., -2.2070e-01,
           1.8768e-03,  1.4648e-01]],

        [[-1.1035e-01,  1.1353e-02,  6.4844e-01,  ..., -9.9219e-01,
          -4.3457e-02,  1.8672e+00],
         [-2.6123e-02,  1.7944e-02, -5.0391e-01,  ..., -4.9023e-01,
           6.8970e-03,  2.4805e-01],
         [-1.4893e-02,  1.3794e-02, -6.4453e-01,  ..., -5.8984e-01,
           5.5542e-03,  3.0859e-01],
         ...,
         [-5.9570e-02, -7.4463e-03, -4.7266e-01,  ...,  4.5508e-01,
          -5.4016e-03,  1.8164e-01],
         [-6.6895e-02, -4.3030e-03, -4.3945e-01,  ...,  8.2422e-01,
          -3.4637e-03,  1.5234e-01],
         [-7.2266e-02,  3.6621e-03, -3.0469e-01,  ...,  1.0312e+00,
           5.2185e-03,  1.8652e-01]],

        [[-3.3203e-02, -9.2773e-03, -6.7188e-01,  ..., -2.1875e-01,
          -1.1230e-02, -1.2500e-01],
         [-4.1504e-02, -1.1536e-02, -7.7344e-01,  ..., -1.2158e-01,
          -1.1780e-02, -1.8750e-01],
         [-3.7109e-02, -2.6550e-03, -9.8438e-01,  ..., -1.0757e-03,
          -3.3264e-03, -2.1875e-01],
         ...,
         [-6.4453e-02,  5.2246e-02, -3.0469e-01,  ...,  9.4141e-01,
           3.8818e-02,  1.5137e-01],
         [-4.3701e-02,  4.3213e-02, -3.0078e-01,  ...,  4.0430e-01,
           4.4434e-02,  4.9219e-01],
         [-4.6631e-02,  3.8818e-02, -3.3691e-02,  ...,  2.4219e-01,
           4.9561e-02,  4.8047e-01]],

        [[-1.9775e-02,  2.2095e-02, -5.2734e-01,  ...,  1.2598e-01,
           1.0437e-02,  1.1084e-01],
         [-2.0874e-02,  3.3203e-02, -5.3906e-01,  ...,  3.5547e-01,
           2.3926e-02,  2.8564e-02],
         [ 2.5024e-02,  3.9307e-02, -6.0156e-01,  ...,  3.7500e-01,
           3.5889e-02, -1.1377e-01],
         ...,
         [-3.4668e-02,  1.1780e-02, -1.8262e-01,  ...,  6.9922e-01,
           3.9795e-02,  1.2891e-01],
         [-4.7363e-02, -6.3782e-03,  2.8516e-01,  ...,  4.4727e-01,
           3.7598e-02, -1.2500e-01],
         [-5.4443e-02, -2.3560e-02,  4.7266e-01,  ...,  6.2500e-02,
           2.8931e-02, -1.8262e-01]]], device='cuda:0', dtype=torch.bfloat16,
       grad_fn=<_DDPSinkBackward>)
Z-Shape: torch.Size([4, 48, 1280])
