INFO:root:called-params configs/in1k_vith14_ep300_finetuning.yaml
INFO:root:loaded params...
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
INFO:root:Running... (rank: 0/8)
INFO:root:Initialized (rank/world-size) 0/8
INFO:root:making imagenet data transforms
INFO:root:making imagenet data transforms
INFO:root:PlantCLEF dataset created
Training dataset, length: 284160
INFO:root:PlantCLEF dataset created
Val dataset, length: 76416
INFO:root:Using AdamW
Mixup is activated!
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:loaded pretrained encoder from epoch 299 with msg: <All keys matched successfully>
INFO:root:loaded optimizers from epoch 299
INFO:root:read-path: /home/rtcalumby/adam/luciano/LifeCLEFPlant2022/IN1K-vit.h.14-300e.pth.tar
INFO:root:Using AdamW
INFO:root:DistributedDataParallel(
  (module): FinetuningModel(
    (pretrained_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14))
      )
      (blocks): ModuleList(
        (0-31): 32 x Block(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (norm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
    )
    (average_pool): AvgPool1d(kernel_size=(256,), stride=(1,), padding=(0,))
    (head_drop): Dropout(p=0.25, inplace=False)
    (mlp_head): Linear(in_features=1280, out_features=80000, bias=True)
  )
)
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:Epoch 1
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
{   'data': {   'batch_size': 64,
                'color_jitter_strength': 0.0,
                'crop_scale': [0.3, 1.0],
                'crop_size': 224,
                'cutmix': 1.0,
                'drop_path': 0.25,
                'image_folder': 'LifeCLEFPlant2022/',
                'mixup': 0.8,
                'nb_classes': 80000,
                'num_workers': 64,
                'pin_mem': True,
                'reprob': 0.25,
                'resume_epoch': 0,
                'root_path': '/home/rtcalumby/adam/luciano/',
                'use_color_distortion': False,
                'use_gaussian_blur': False,
                'use_horizontal_flip': False},
    'logging': {   'folder': '/home/rtcalumby/adam/luciano/LifeCLEFPlant2022/logs/inet_feature_extraction',
                   'write_tag': 'jepa'},
    'mask': {   'allow_overlap': False,
                'aspect_ratio': [0.75, 1.5],
                'enc_mask_scale': [0.85, 1.0],
                'min_keep': 10,
                'num_enc_masks': 1,
                'num_pred_masks': 4,
                'patch_size': 14,
                'pred_mask_scale': [0.15, 0.2]},
    'meta': {   'copy_data': False,
                'load_checkpoint': True,
                'model_name': 'vit_huge',
                'pred_depth': 12,
                'pred_emb_dim': 384,
                'read_checkpoint': None,
                'use_bfloat16': False},
    'optimization': {   'ema': [0.9, 0.999],
                        'epochs': 50,
                        'final_lr': 1e-07,
                        'final_weight_decay': 0.4,
                        'ipe_scale': 1.0,
                        'label_smoothing': 0.1,
                        'lr': 0.00025,
                        'start_lr': 0.00025,
                        'warmup': 10,
                        'weight_decay': 0.05}}
Training dataset, length: 284160
Val dataset, length: 76416
Mixup is activated!
['encoder', 'predictor', 'opt', 'scaler', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr']
INFO:root:[1,     0/ 4440] train_loss: 11.290 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 5.58e+04] (12169.5 ms)
INFO:root:[1,     0] grad_stats: [2.76e-05 2.91e-04] (1.06e-05, 3.85e-01)
INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.
INFO:root:[1,   200/ 4440] train_loss: 11.180 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3876.9 ms)
INFO:root:[1,   200] grad_stats: [6.12e-05 1.72e-04] (1.59e-06, 3.95e-01)
INFO:root:[1,   400/ 4440] train_loss: 11.126 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3855.1 ms)
INFO:root:[1,   400] grad_stats: [7.56e-05 1.29e-04] (1.60e-06, 3.84e-01)
INFO:root:[1,   600/ 4440] train_loss: 11.104 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3847.8 ms)
INFO:root:[1,   600] grad_stats: [3.30e-05 1.07e-04] (1.92e-06, 4.10e-01)
INFO:root:[1,   800/ 4440] train_loss: 11.085 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3844.2 ms)
INFO:root:[1,   800] grad_stats: [4.30e-05 6.24e-05] (3.64e-06, 3.90e-01)
INFO:root:[1,  1000/ 4440] train_loss: 11.069 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3842.1 ms)
INFO:root:[1,  1000] grad_stats: [2.85e-05 9.64e-05] (3.16e-06, 4.23e-01)
INFO:root:[1,  1200/ 4440] train_loss: 11.055 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3840.7 ms)
INFO:root:[1,  1200] grad_stats: [2.38e-05 6.72e-05] (2.55e-06, 3.78e-01)
INFO:root:[1,  1400/ 4440] train_loss: 11.042 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3839.6 ms)
INFO:root:[1,  1400] grad_stats: [5.23e-05 5.89e-05] (2.55e-06, 3.86e-01)
INFO:root:[1,  1600/ 4440] train_loss: 11.032 [wd: 5.00e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.8 ms)
INFO:root:[1,  1600] grad_stats: [1.64e-05 5.91e-05] (3.04e-06, 3.87e-01)
INFO:root:[1,  1800/ 4440] train_loss: 11.023 [wd: 5.01e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.2 ms)
INFO:root:[1,  1800] grad_stats: [3.82e-05 8.02e-05] (3.63e-06, 3.82e-01)
INFO:root:[1,  2000/ 4440] train_loss: 11.015 [wd: 5.01e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.7 ms)
INFO:root:[1,  2000] grad_stats: [1.34e-04 9.02e-05] (6.11e-06, 3.34e-01)
INFO:root:[1,  2200/ 4440] train_loss: 11.009 [wd: 5.01e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[1,  2200] grad_stats: [1.29e-04 8.20e-05] (4.09e-06, 3.90e-01)
INFO:root:[1,  2400/ 4440] train_loss: 11.003 [wd: 5.01e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.0 ms)
INFO:root:[1,  2400] grad_stats: [4.88e-04 8.88e-05] (7.13e-06, 3.85e-01)
INFO:root:[1,  2600/ 4440] train_loss: 10.998 [wd: 5.01e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.7 ms)
INFO:root:[1,  2600] grad_stats: [4.74e-04 6.57e-05] (4.58e-06, 3.67e-01)
INFO:root:[1,  2800/ 4440] train_loss: 10.992 [wd: 5.01e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.5 ms)
INFO:root:[1,  2800] grad_stats: [9.83e-04 8.29e-05] (4.43e-06, 4.16e-01)
INFO:root:[1,  3000/ 4440] train_loss: 10.987 [wd: 5.02e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.3 ms)
INFO:root:[1,  3000] grad_stats: [2.13e-03 8.25e-05] (1.30e-05, 3.76e-01)
INFO:root:[1,  3200/ 4440] train_loss: 10.982 [wd: 5.02e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.1 ms)
INFO:root:[1,  3200] grad_stats: [3.11e-03 1.42e-04] (1.12e-05, 3.54e-01)
INFO:root:[1,  3400/ 4440] train_loss: 10.977 [wd: 5.02e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[1,  3400] grad_stats: [2.20e-03 7.25e-05] (1.01e-05, 4.01e-01)
INFO:root:[1,  3600/ 4440] train_loss: 10.972 [wd: 5.02e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[1,  3600] grad_stats: [1.92e-03 8.79e-05] (7.72e-06, 4.22e-01)
INFO:root:[1,  3800/ 4440] train_loss: 10.968 [wd: 5.03e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.8 ms)
INFO:root:[1,  3800] grad_stats: [2.01e-03 8.89e-05] (7.97e-06, 4.08e-01)
INFO:root:[1,  4000/ 4440] train_loss: 10.963 [wd: 5.03e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.7 ms)
INFO:root:[1,  4000] grad_stats: [2.96e-03 1.29e-04] (1.13e-05, 4.25e-01)
INFO:root:[1,  4200/ 4440] train_loss: 10.959 [wd: 5.03e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.6 ms)
INFO:root:[1,  4200] grad_stats: [3.99e-03 1.15e-04] (1.38e-05, 4.00e-01)
INFO:root:[1,  4400/ 4440] train_loss: 10.955 [wd: 5.03e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.5 ms)
INFO:root:[1,  4400] grad_stats: [2.18e-03 8.33e-05] (5.97e-06, 3.46e-01)
Loss: 10.918336868286133
Loss: 10.918336868286133
Loss: 10.918336868286133
Loss: 10.918336868286133
INFO:root:avg. train_loss 10.954
INFO:root:avg. test_loss 11.911 avg. Accuracy@1 0.007 - avg. Accuracy@5 0.012
Loss: 10.918336868286133
Loss: 10.918336868286133
Loss: 10.918336868286133
Loss: 10.918336868286133
INFO:root:Epoch 2
INFO:root:[2,     0/ 4440] train_loss: 10.862 [wd: 5.03e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3862.3 ms)
INFO:root:[2,     0] grad_stats: [3.08e-03 1.32e-04] (1.06e-05, 4.14e-01)
INFO:root:[2,   200/ 4440] train_loss: 10.837 [wd: 5.04e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.1 ms)
INFO:root:[2,   200] grad_stats: [2.15e-03 9.25e-05] (9.95e-06, 3.94e-01)
INFO:root:[2,   400/ 4440] train_loss: 10.840 [wd: 5.04e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,   400] grad_stats: [3.94e-03 1.17e-04] (1.45e-05, 3.84e-01)
INFO:root:[2,   600/ 4440] train_loss: 10.840 [wd: 5.04e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,   600] grad_stats: [3.66e-03 1.43e-04] (2.12e-05, 3.79e-01)
INFO:root:[2,   800/ 4440] train_loss: 10.839 [wd: 5.05e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,   800] grad_stats: [4.67e-03 1.33e-04] (2.78e-05, 3.54e-01)
INFO:root:[2,  1000/ 4440] train_loss: 10.836 [wd: 5.05e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  1000] grad_stats: [5.56e-03 1.92e-04] (2.36e-05, 4.12e-01)
INFO:root:[2,  1200/ 4440] train_loss: 10.833 [wd: 5.06e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,  1200] grad_stats: [4.27e-03 1.14e-04] (1.87e-05, 3.79e-01)
INFO:root:[2,  1400/ 4440] train_loss: 10.829 [wd: 5.06e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,  1400] grad_stats: [6.66e-03 1.76e-04] (3.46e-05, 4.02e-01)
INFO:root:[2,  1600/ 4440] train_loss: 10.825 [wd: 5.06e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  1600] grad_stats: [6.49e-03 2.18e-04] (3.86e-05, 4.17e-01)
INFO:root:[2,  1800/ 4440] train_loss: 10.821 [wd: 5.07e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,  1800] grad_stats: [6.58e-03 1.99e-04] (3.07e-05, 3.82e-01)
INFO:root:[2,  2000/ 4440] train_loss: 10.817 [wd: 5.07e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,  2000] grad_stats: [6.45e-03 2.79e-04] (3.74e-05, 3.79e-01)
INFO:root:[2,  2200/ 4440] train_loss: 10.812 [wd: 5.08e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.8 ms)
INFO:root:[2,  2200] grad_stats: [5.81e-03 2.82e-04] (3.11e-05, 3.97e-01)
INFO:root:[2,  2400/ 4440] train_loss: 10.807 [wd: 5.08e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  2400] grad_stats: [7.47e-03 3.79e-04] (4.33e-05, 4.14e-01)
INFO:root:[2,  2600/ 4440] train_loss: 10.802 [wd: 5.09e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  2600] grad_stats: [7.65e-03 2.01e-04] (2.93e-05, 3.71e-01)
INFO:root:[2,  2800/ 4440] train_loss: 10.797 [wd: 5.09e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  2800] grad_stats: [8.40e-03 4.80e-04] (3.83e-05, 3.82e-01)
INFO:root:[2,  3000/ 4440] train_loss: 10.793 [wd: 5.10e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  3000] grad_stats: [1.26e-02 2.64e-04] (3.84e-05, 4.21e-01)
INFO:root:[2,  3200/ 4440] train_loss: 10.788 [wd: 5.10e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  3200] grad_stats: [1.07e-02 2.65e-04] (4.76e-05, 3.98e-01)
INFO:root:[2,  3400/ 4440] train_loss: 10.783 [wd: 5.11e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3834.9 ms)
INFO:root:[2,  3400] grad_stats: [7.46e-03 4.00e-04] (4.65e-05, 3.55e-01)
INFO:root:[2,  3600/ 4440] train_loss: 10.778 [wd: 5.11e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.0 ms)
INFO:root:[2,  3600] grad_stats: [1.06e-02 3.38e-04] (3.81e-05, 3.90e-01)
INFO:root:[2,  3800/ 4440] train_loss: 10.774 [wd: 5.12e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.0 ms)
INFO:root:[2,  3800] grad_stats: [9.23e-03 3.63e-04] (6.34e-05, 3.44e-01)
INFO:root:[2,  4000/ 4440] train_loss: 10.769 [wd: 5.12e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.0 ms)
INFO:root:[2,  4000] grad_stats: [1.06e-02 6.70e-04] (8.00e-05, 4.15e-01)
INFO:root:[2,  4200/ 4440] train_loss: 10.765 [wd: 5.13e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.1 ms)
INFO:root:[2,  4200] grad_stats: [1.16e-02 6.87e-04] (7.93e-05, 3.88e-01)
INFO:root:[2,  4400/ 4440] train_loss: 10.761 [wd: 5.14e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.1 ms)
INFO:root:[2,  4400] grad_stats: [9.84e-03 4.86e-04] (5.80e-05, 3.82e-01)
Loss: 10.72900104522705
Loss: 10.72900104522705
Loss: 10.72900104522705
Loss: 10.72900104522705
Loss: 10.72900104522705
INFO:root:avg. train_loss 10.760
INFO:root:avg. test_loss 12.101 avg. Accuracy@1 0.000 - avg. Accuracy@5 0.005
Loss: 10.72900104522705
Loss: 10.72900104522705
Loss: 10.72900104522705
INFO:root:Epoch 3
INFO:root:[3,     0/ 4440] train_loss: 10.475 [wd: 5.14e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3855.5 ms)
INFO:root:[3,     0] grad_stats: [1.33e-02 6.48e-04] (7.78e-05, 4.22e-01)
INFO:root:[3,   200/ 4440] train_loss: 10.624 [wd: 5.14e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.3 ms)
INFO:root:[3,   200] grad_stats: [1.27e-02 3.93e-04] (8.08e-05, 4.04e-01)
INFO:root:[3,   400/ 4440] train_loss: 10.626 [wd: 5.15e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.3 ms)
INFO:root:[3,   400] grad_stats: [1.14e-02 6.16e-04] (5.46e-05, 3.90e-01)
INFO:root:[3,   600/ 4440] train_loss: 10.629 [wd: 5.16e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.1 ms)
INFO:root:[3,   600] grad_stats: [1.17e-02 3.74e-04] (5.32e-05, 3.82e-01)
INFO:root:[3,   800/ 4440] train_loss: 10.626 [wd: 5.16e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,   800] grad_stats: [1.14e-02 4.32e-04] (6.77e-05, 4.00e-01)
INFO:root:[3,  1000/ 4440] train_loss: 10.624 [wd: 5.17e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  1000] grad_stats: [1.24e-02 5.09e-04] (7.80e-05, 4.27e-01)
INFO:root:[3,  1200/ 4440] train_loss: 10.623 [wd: 5.18e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.8 ms)
INFO:root:[3,  1200] grad_stats: [1.05e-02 5.17e-04] (7.17e-05, 3.98e-01)
INFO:root:[3,  1400/ 4440] train_loss: 10.621 [wd: 5.18e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.8 ms)
INFO:root:[3,  1400] grad_stats: [1.73e-02 6.93e-04] (1.10e-04, 3.73e-01)
INFO:root:[3,  1600/ 4440] train_loss: 10.618 [wd: 5.19e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.8 ms)
INFO:root:[3,  1600] grad_stats: [1.42e-02 6.84e-04] (1.53e-04, 3.81e-01)
INFO:root:[3,  1800/ 4440] train_loss: 10.615 [wd: 5.20e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  1800] grad_stats: [1.28e-02 6.38e-04] (1.02e-04, 3.82e-01)
INFO:root:[3,  2000/ 4440] train_loss: 10.613 [wd: 5.21e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  2000] grad_stats: [1.33e-02 5.89e-04] (7.54e-05, 3.89e-01)
INFO:root:[3,  2200/ 4440] train_loss: 10.610 [wd: 5.21e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  2200] grad_stats: [1.41e-02 9.69e-04] (1.31e-04, 4.18e-01)
INFO:root:[3,  2400/ 4440] train_loss: 10.607 [wd: 5.22e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  2400] grad_stats: [1.91e-02 8.19e-04] (1.03e-04, 4.15e-01)
INFO:root:[3,  2600/ 4440] train_loss: 10.606 [wd: 5.23e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  2600] grad_stats: [1.74e-02 7.54e-04] (1.31e-04, 3.86e-01)
INFO:root:[3,  2800/ 4440] train_loss: 10.603 [wd: 5.24e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3835.9 ms)
INFO:root:[3,  2800] grad_stats: [1.54e-02 6.18e-04] (1.23e-04, 3.95e-01)
INFO:root:[3,  3000/ 4440] train_loss: 10.600 [wd: 5.25e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,  3000] grad_stats: [1.30e-02 6.83e-04] (1.16e-04, 3.40e-01)
INFO:root:[3,  3200/ 4440] train_loss: 10.597 [wd: 5.26e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,  3200] grad_stats: [1.68e-02 6.07e-04] (1.06e-04, 3.83e-01)
INFO:root:[3,  3400/ 4440] train_loss: 10.593 [wd: 5.26e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,  3400] grad_stats: [1.26e-02 7.59e-04] (1.14e-04, 3.71e-01)
INFO:root:[3,  3600/ 4440] train_loss: 10.589 [wd: 5.27e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,  3600] grad_stats: [2.34e-02 8.34e-04] (1.76e-04, 3.80e-01)
INFO:root:[3,  3800/ 4440] train_loss: 10.587 [wd: 5.28e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,  3800] grad_stats: [1.45e-02 7.44e-04] (1.44e-04, 3.89e-01)
INFO:root:[3,  4000/ 4440] train_loss: 10.584 [wd: 5.29e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.0 ms)
INFO:root:[3,  4000] grad_stats: [1.59e-02 6.82e-04] (1.02e-04, 3.55e-01)
INFO:root:[3,  4200/ 4440] train_loss: 10.582 [wd: 5.30e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.1 ms)
INFO:root:[3,  4200] grad_stats: [2.18e-02 7.80e-04] (1.47e-04, 3.98e-01)
INFO:root:[3,  4400/ 4440] train_loss: 10.580 [wd: 5.31e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.1 ms)
INFO:root:[3,  4400] grad_stats: [1.17e-02 9.46e-04] (1.17e-04, 3.90e-01)
Loss: 10.298678398132324
Loss: 10.298678398132324
Loss: 10.298678398132324
INFO:root:avg. train_loss 10.579
INFO:root:avg. test_loss nan avg. Accuracy@1 0.000 - avg. Accuracy@5 0.004
Loss: 10.298678398132324
Loss: 10.298678398132324
Loss: 10.298678398132324
Loss: 10.298678398132324
Loss: 10.298678398132324
INFO:root:Epoch 4
INFO:root:[4,     0/ 4440] train_loss: 10.649 [wd: 5.31e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3856.1 ms)
INFO:root:[4,     0] grad_stats: [2.14e-02 8.85e-04] (1.50e-04, 3.69e-01)
INFO:root:[4,   200/ 4440] train_loss: 10.477 [wd: 5.32e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.1 ms)
INFO:root:[4,   200] grad_stats: [2.33e-02 1.23e-03] (2.49e-04, 4.05e-01)
INFO:root:[4,   400/ 4440] train_loss: 10.471 [wd: 5.33e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.9 ms)
INFO:root:[4,   400] grad_stats: [1.62e-02 9.53e-04] (1.46e-04, 4.01e-01)
INFO:root:[4,   600/ 4440] train_loss: 10.475 [wd: 5.34e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.9 ms)
INFO:root:[4,   600] grad_stats: [1.97e-02 9.08e-04] (1.70e-04, 4.25e-01)
INFO:root:[4,   800/ 4440] train_loss: 10.476 [wd: 5.35e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3836.9 ms)
INFO:root:[4,   800] grad_stats: [2.37e-02 1.03e-03] (1.85e-04, 4.34e-01)
INFO:root:[4,  1000/ 4440] train_loss: 10.477 [wd: 5.36e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.0 ms)
INFO:root:[4,  1000] grad_stats: [1.96e-02 8.43e-04] (1.46e-04, 4.01e-01)
INFO:root:[4,  1200/ 4440] train_loss: 10.480 [wd: 5.37e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.1 ms)
INFO:root:[4,  1200] grad_stats: [1.76e-02 1.19e-03] (1.47e-04, 4.08e-01)
INFO:root:[4,  1400/ 4440] train_loss: 10.477 [wd: 5.38e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.2 ms)
INFO:root:[4,  1400] grad_stats: [2.97e-02 1.22e-03] (2.15e-04, 4.17e-01)
INFO:root:[4,  1600/ 4440] train_loss: 10.476 [wd: 5.39e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  1600] grad_stats: [1.42e-02 1.04e-03] (1.40e-04, 3.69e-01)
INFO:root:[4,  1800/ 4440] train_loss: 10.474 [wd: 5.40e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  1800] grad_stats: [2.22e-02 1.29e-03] (2.06e-04, 4.36e-01)
INFO:root:[4,  2000/ 4440] train_loss: 10.475 [wd: 5.41e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.4 ms)
INFO:root:[4,  2000] grad_stats: [2.75e-02 9.81e-04] (2.33e-04, 3.98e-01)
INFO:root:[4,  2200/ 4440] train_loss: 10.473 [wd: 5.42e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.4 ms)
INFO:root:[4,  2200] grad_stats: [2.43e-02 8.75e-04] (2.07e-04, 4.02e-01)
INFO:root:[4,  2400/ 4440] train_loss: 10.472 [wd: 5.43e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  2400] grad_stats: [2.01e-02 1.11e-03] (2.15e-04, 4.14e-01)
INFO:root:[4,  2600/ 4440] train_loss: 10.472 [wd: 5.44e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  2600] grad_stats: [2.66e-02 1.27e-03] (2.68e-04, 4.28e-01)
INFO:root:[4,  2800/ 4440] train_loss: 10.470 [wd: 5.45e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  2800] grad_stats: [2.08e-02 1.45e-03] (2.38e-04, 3.93e-01)
INFO:root:[4,  3000/ 4440] train_loss: 10.469 [wd: 5.46e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  3000] grad_stats: [1.87e-02 9.13e-04] (1.70e-04, 3.99e-01)
INFO:root:[4,  3200/ 4440] train_loss: 10.468 [wd: 5.48e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  3200] grad_stats: [1.93e-02 1.09e-03] (1.78e-04, 3.57e-01)
INFO:root:[4,  3400/ 4440] train_loss: 10.466 [wd: 5.49e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  3400] grad_stats: [4.24e-02 1.07e-03] (1.92e-04, 3.85e-01)
INFO:root:[4,  3600/ 4440] train_loss: 10.465 [wd: 5.50e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  3600] grad_stats: [1.93e-02 1.00e-03] (2.11e-04, 3.94e-01)
INFO:root:[4,  3800/ 4440] train_loss: 10.463 [wd: 5.51e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  3800] grad_stats: [1.92e-02 1.05e-03] (2.35e-04, 3.87e-01)
INFO:root:[4,  4000/ 4440] train_loss: 10.462 [wd: 5.52e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  4000] grad_stats: [3.24e-02 1.23e-03] (2.27e-04, 3.74e-01)
INFO:root:[4,  4200/ 4440] train_loss: 10.461 [wd: 5.54e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  4200] grad_stats: [2.79e-02 1.60e-03] (2.60e-04, 4.07e-01)
INFO:root:[4,  4400/ 4440] train_loss: 10.459 [wd: 5.55e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3837.3 ms)
INFO:root:[4,  4400] grad_stats: [2.81e-02 1.53e-03] (2.48e-04, 4.04e-01)
Loss: 10.546560287475586
Loss: 10.546560287475586
Loss: 10.546560287475586
Loss: 10.546560287475586
INFO:root:avg. train_loss 10.459
INFO:root:avg. test_loss 12.226 avg. Accuracy@1 0.001 - avg. Accuracy@5 0.007
Loss: 10.546560287475586
Loss: 10.546560287475586
Loss: 10.546560287475586
Loss: 10.546560287475586
INFO:root:Epoch 5
INFO:root:[5,     0/ 4440] train_loss: 10.402 [wd: 5.55e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3868.8 ms)
INFO:root:[5,     0] grad_stats: [1.77e-02 9.83e-04] (1.75e-04, 3.72e-01)
INFO:root:[5,   200/ 4440] train_loss: 10.367 [wd: 5.56e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.5 ms)
INFO:root:[5,   200] grad_stats: [2.56e-02 1.41e-03] (2.20e-04, 3.96e-01)
INFO:root:[5,   400/ 4440] train_loss: 10.379 [wd: 5.57e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.1 ms)
INFO:root:[5,   400] grad_stats: [3.36e-02 1.37e-03] (3.10e-04, 4.02e-01)
INFO:root:[5,   600/ 4440] train_loss: 10.385 [wd: 5.59e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,   600] grad_stats: [3.83e-02 1.56e-03] (2.79e-04, 3.89e-01)
INFO:root:[5,   800/ 4440] train_loss: 10.386 [wd: 5.60e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,   800] grad_stats: [2.84e-02 1.33e-03] (2.30e-04, 3.91e-01)
INFO:root:[5,  1000/ 4440] train_loss: 10.382 [wd: 5.61e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  1000] grad_stats: [4.09e-02 1.38e-03] (3.21e-04, 4.17e-01)
INFO:root:[5,  1200/ 4440] train_loss: 10.384 [wd: 5.63e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  1200] grad_stats: [3.25e-02 1.35e-03] (3.03e-04, 4.19e-01)
INFO:root:[5,  1400/ 4440] train_loss: 10.382 [wd: 5.64e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  1400] grad_stats: [3.30e-02 1.94e-03] (4.04e-04, 4.03e-01)
INFO:root:[5,  1600/ 4440] train_loss: 10.383 [wd: 5.65e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  1600] grad_stats: [2.46e-02 1.20e-03] (2.16e-04, 3.82e-01)
INFO:root:[5,  1800/ 4440] train_loss: 10.383 [wd: 5.67e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  1800] grad_stats: [2.16e-02 1.27e-03] (2.67e-04, 4.22e-01)
INFO:root:[5,  2000/ 4440] train_loss: 10.383 [wd: 5.68e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  2000] grad_stats: [2.61e-02 1.19e-03] (2.27e-04, 3.65e-01)
INFO:root:[5,  2200/ 4440] train_loss: 10.382 [wd: 5.69e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  2200] grad_stats: [3.81e-02 1.84e-03] (3.77e-04, 4.08e-01)
INFO:root:[5,  2400/ 4440] train_loss: 10.382 [wd: 5.71e-02] [lr: 2.50e-04] [mem: 6.41e+04] (3838.0 ms)
INFO:root:[5,  2400] grad_stats: [1.89e-02 1.54e-03] (2.62e-04, 3.96e-01)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 2134.0 ON hgx CANCELLED AT 2024-05-25T15:32:25 ***
srun: error: hgx: task 0: Terminated
